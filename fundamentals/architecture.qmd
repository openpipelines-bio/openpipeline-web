---
title: Architecture
order: 30
---

Openpipeline is a pipeline for the processing of multimodal single-cell data that scales to a great many of samples. Covering the architecture requires us to explain many angles: what the expected inputs and outputs are for each workflow are, how do the workflows relate to each other, what the state of the data is at each step of the pipeline, etc... Here is an overview of the general steps involved in processing sequencing data into a single integrated object. We will discuss each of the steps further below.

```{mermaid}
%%| label: fig-architecture
%%| fig-cap: Overview of the steps included in OpenPipeline for the analysis of single cell multiomics data.
flowchart TD  
  ingest["Ingestion"] --> split --> unimodalsinglesample["Unimodal Single Sample Processing"] --> concat --> unimodalmultisample["Unimodal Multi Sample Processing"] --> merging --> integation_setup["Integration Setup"] --> integration["Integration"]  --> downstreamprocessing["Downstream Processing"]
```

1. [Ingestion](#ingestion): Convert raw sequencing data or count tables into MuData data for further processing.
2. [Splitting modalities](#sec-splitting): Creating several MuData objects, one per modality, out of a multimodal input sample.
3. [Unimodal Single Sample Processing](#sec-single-sample): tools applied to each modality of samples individually. Mostly involes the selection of true from false cells.
4. [Unimodal Multi Sample Processing](#sec-multisample-processing): steps that require information from all samples together. Processing is still performed per-modality.
5. [Merging](#sec-merging): Creating one MuData object from several unimodal MuData input files. 
6. [Initializing Integration](#sec-initializing-integration): Performs dimensionality reduction and cell type clustering on non-integrated samples. These are popular steps that would otherwise be executed manually or they provide input for downstream integration methods.
7. [Integration](#sec-intergration): The alignment of cell types across samples. Can be performed per modality or based on multiple modalities.
8. Downstream Processing: Extra analyses performed on the integrated dataset and conversion to other file formats.

# Available workflows
The structure of the sections that have been laid out below follow a logical grouping of the processing according to the state _of the data_. However, even though this grouping makes sense from a data perspective, it does not mean that a workflow exist for each section. For example, the [processing a single sample](#sec-single-sample) section describes how processing of single sample is performed as part of the [full pipeline](#sec-full-pipeline), but there is no `singlesample` workflow that a user can execute. The inverse is also possible: while there exists an [multisample](../components/workflows/multiomics/mutlisample.qmd) pipeline, it's functionality is not limited to what has been described in the section [Multisample Processing](#sec-multisample-processing). This section lists all the available workflows and will try to describe the link with the relevant sections below.

## Ingestion workflows
All of the following workflows from the ingestion namespace have been discussed in more detail in the [ingestion](#sec-ingestion) section:

* [ingestion/bd_rhapsody](../components/workflows/ingestion/bd_rhapsody.qmd)
* [ingestion/cellranger_mapping](../components/workflows/ingestion/cellranger_mapping.qmd)
* [ingestion/cellranger_multi](../components/workflows/ingestion/cellranger_multi.qmd)
* [ingestion/demux](../components/workflows/ingestion/demux.qmd)
* [ingestion/make_reference](../components/workflows/ingestion/make_reference.qmd) 


## Multiomics workflows
There exists no `singlesample` workflow. However, the `prot_singlesample` and `rna_singlesample` pipelines do exist and they map identically to the functionality described in the [single-sample antibody capture Processing](#sec-single-sample-adt).qmd and [single-sample gene Expression processing](#sec-single-sample-gex) sections respectively. If you would like to process your samples as described in the [unimodal single sample processing](#sec-single-sample) section, you can execute both workflows in tandem for the two modalities.

Contrary to the workflows for single sample processing, there exists a [multiomics/multisample](../components/workflows/multiomics/multisample.qmd) workflow. However this workflow is not just the [multiomics/prot_multisample](../components/workflows/multiomics/prot_multisample.qmd) and [multiomics/rna_multisample](../components/workflows/multiomics/rna_multisample.qmd) workflows that have been combined. Instead, it combines the [multiomics/prot_multisample](../components/workflows/multiomics/prot_multisample.qmd), [multiomics/rna_multisample](../components/workflows/multiomics/rna_multisample.qmd) and [multiomics/integration/initialize_integration](../components/workflows/multiomics/integration/initialize_integration.qmd) workflows. The purpose of this pipeline is to provide an extra 'entrypoint' into the full pipeline that skips the singlesample processing, allowing reprocessing samples that have already been processed before. A popular usecase is to manually select one or more celltypes which need to be processed again or the integration of observations from multiple experiments into a single dataset. Keep in mind that concatenation is not included in the multisample pipeline, so when multiple input files are specified they are processed in parallel. If you would like to integrate multiple experiments, you need to first concatenate them in a seperate step:

```{.d2}
file_input_1: "Experiment 1\n(multisample)" {
  shape: page
}

file_input_2: "Experiment 2\n(multisample)" {
  shape: page
}

file_output: "Output" {
  shape: page
}

pipeline_out_1: "Full pipeline \\ integration \\..." {
  shape: parallelogram
  style.stroke-dash: 5
}

pipeline_out_2: "Full pipeline \\ integration  \\..." {
  shape: parallelogram
  style.stroke-dash: 5
}

concat: "Concatenation" {
  shape: parallelogram
}

multisample: "Multisample" {
  shape: parallelogram
}

pipeline_out_1 -> file_input_1
pipeline_out_2 -> file_input_2
file_input_1 -> concat <- file_input_2

concat -> multisample -> file_output

style: {
  fill: "#FCFCFC"
}
```

## The "full" pipeline
The name of this pipeline is a bit of a misnomer, because it does not include all the steps from ingestion to integration. As will be discussed in the [ingestion](#sec-ingestion) section, which ingestion strategy you need is dependant on your technology provider and the chosen platform. For [integration](#sec-integration-methods), there exist many methods and combination of methods, and you may wish to choose which integration methods are applicable for your usecase. As a consequence, these two stages in the analysis of single-cell need to be executed seperatly and not as part of a single unified pipeline. All other steps outlined below on the other hand are included into the "full" pipeline, which can therefore be summarized in the following figure:

```{mermaid}
%%| label: fig-full-pipeline
%%| fig-cap: Overview of the steps included in the full pipelines from OpenPipeline.
flowchart TD  
  split --> unimodalsinglesample["Unimodal Single Sample Processing"] --> concat --> unimodalmultisample["Unimodal Multi Sample Processing"] --> merging --> integation_setup
```

## Integration workflows
For each of the integration methods (and their optional combination with other tools), a seperate pipeline is defined. More information for each of the pipelines is available in the [integration methods section](#sec-integration-methods).

* [multiomics/integration/bbknn_leiden](../components/workflows/multiomics/integration/bbknn_leiden.qmd)
* [multiomics/integration/harmony_leiden](../components/workflows/multiomics/integration/harmony_leiden.qmd)
* [multiomics/integration/scanorama_leiden](../components/workflows/multiomics/integration/scanorama_leiden.qmd)
* [multiomics/integration/scvi_leiden](../components/workflows/multiomics/integration/scvi_leiden.qmd)
* [multiomics/integration/totalvi_leiden](../components/workflows/multiomics/integration/totalvi_leiden.qmd)
* [multiomics/integration/initialize_integration](../components/workflows/multiomics/integration/initialize_integration.qmd)


# Important dataflow components
While most components included in openpipelines are involved in data analysis, the sole purpose of other components is to facilitate data flow throughout the pipelines. In a workflow, output for a component is written to disk after it is done performing its task, and is read back in by the next component. However, the relation between the component and the next component is not always a clear one to one relationship. For example: some tools are capable of analyzing a single sample, while others require the input of all samples together. Additionally, not only are the input requirement of tools limiting, performance also needs to be taken into account. Tasks which are performed on each sample separately can be executed in parallel, while if the same task is performed on a single file that contains the data for all samples. In order to facilitate one-to-many or many-to-one relations between components and to allow parallel execution of tasks, component that are specialized in dataflow were implemented.  

## Splitting modalities {#sec-splitting}
We refer to splitting modalities when multimodal MuData file is split into several unimodal MuData files. The number of output files is equal to the number of modalities present in the input file. Splitting the modalities works on MuData files containing data for multiple samples or for single-sample files.

~~~{.d2 layout=elk}
file_input: MuData Input{
  shape: page
}

file_input.mudata_input: |||md
  ```
  └─.mod
     └─ rna
     └─ prot
     └─ ...
  ```
|||
file_input.style.font-size: 20

file_output_rna: MuData Output\nGene Expression{
  shape: page
}
file_output_rna.mudata_output_rna: |||md
  ```
  └─.mod
     └─ rna
  ```
|||
file_output_rna.style.font-size: 20

file_output_prot: MuData Output\nAntibody Capture{
  shape: page
}
file_output_prot.mudata_output_prot: |||md
  ```
  └─.mod
     └─ prot
  ```
|||
file_output_prot.style.font-size: 20

file_output_other: MuData Output\nOther{
  shape: page
}
file_output_other.mudata_output_other: |||md
  ```
  └─.mod
     └─ ...
  ```
|||
file_output_other.style.font-size: 20

split_modalities: Split Modalities {
  shape: parallelogram
}


file_input -> split_modalities
split_modalities -> file_output_rna
split_modalities -> file_output_prot
split_modalities -> file_output_other

style: {
  fill: "#FCFCFC"
}

~~~

## Merging of modalities {#sec-merging}
Merging refers to combining multiple files with data for one modality into a single output file that contains all input modalities. It is the inverse operation of splitting the modalities.

~~~{.d2 layout=elk}
file_output: MuData Output{
  shape: page
}

file_output.mudata_input: |||md
  ```
  └─.mod
     └─ rna
     └─ prot
     └─ ...
  ```
|||
file_output.style.font-size: 20

file_input_rna: MuData Input\nGene Expression{
  shape: page
}
file_input_rna.mudata_input_rna: |||md
  ```
  └─.mod
     └─ rna
  ```
|||
file_input_rna.style.font-size: 20

file_input_prot: MuData Input\nAntibody Capture{
  shape: page
}
file_input_prot.mudata_input_prot: |||md
  ```
  └─.mod
     └─ prot
  ```
|||
file_input_prot.style.font-size: 20

file_input_other: "MuData Input\nOther"{
  shape: page
}
file_input_other.mudata_input_other: |||md
  ```
  └─.mod
     └─ ...
  ```
|||
file_input_other.style.font-size: 20

merge_modalities: Merge Modalities {
  shape: parallelogram
}

file_input_rna -> merge_modalities
file_input_prot -> merge_modalities
file_input_other -> merge_modalities
merge_modalities -> file_output

style: {
  fill: "#FCFCFC"
}

~~~

## Concatenation of samples

Joining of observations for different samples, stored in their respective MuData file, into a single MuData file for all samples together is called sample concatenation. In practice, this operation is performed for each modality separately. An extra column (with default name `sample_id`) is added to the annotation of the observations (`.obs`) to indicate where each observation originated from.

~~~{.d2 layout=elk}
file_output: MuData Output{
  shape: page
}

file_output.mudata_input: |||md
  ```
  └─.mod
     └─ rna
     └─ prot
     └─ vdj
  
  └─.obs
    [sample_id]
  ```
|||
file_output.style.font-size: 20

file_input_sample1: "MuData Input\nSample 1"{
  shape: page
}
file_input_sample1.mudata_input_sample1: |||md
  ```
  └─.mod
     └─ rna
     └─ vdj
  ```
|||
file_input_sample1.style.font-size: 20

file_input_sample2: "MuData Input\nSample 2"{
  shape: page
}
file_input_sample2.mudata_input_prot: |||md
  ```
  └─.mod
     └─ rna
     └─ prot
  ```
|||
file_input_sample2.style.font-size: 20

file_input_sample3: "MuData Input\nSample 3"{
  shape: page
}
file_input_sample3.mudata_input_sample3: |||md
  ```
  └─.mod
     └─ rna
  ```
|||
file_input_sample3.style.font-size: 20

concatenate_samples: Concatenation {
  shape: parallelogram
}

file_input_sample1 -> concatenate_samples
file_input_sample2 -> concatenate_samples
file_input_sample3 -> concatenate_samples
concatenate_samples -> file_output

style: {
  fill: "#FCFCFC"
}
~~~

Special care must be taken when considering annotations for observations and features while concatenating the samples. Indeed, the data from different samples can contain conflicting information. Openpipeline's `concat` component provides an argument `other_axis_mode` that allows a user to specify what happens when conflicting information is found. The `move` option for this argument is the default behavior. In this mode, each annotation column (from `.obs` and `.var`) is compared across samples. When no conflicts are found or the column is unique for a sample, the column is added output object. When a conflict does occur, all of the columns are gathered from the samples and stored into a dataframe. This dataframe is then stored into `.obsm` for annotations for the observations and `.varm` for feature annotations. This way, a user can have a look at the conflicts and decide what to do with them.

# Ingestion {#sec-ingestion}

Ingestion is the conversion of raw sequencing data or count tables into MuData objects that can be used for further processing.

```{mermaid}
flowchart LR
    RawCounts1["Raw counts"]
    BCL[/"BCL"/]
    Demux[/"Demultiplexing"/]
    Fastq["Fastq"]
    Ref["Reference"]
    Mapping[/"Mapping"/]
    RawDir["Raw out"]
    Convert[/"Convert"/]
    RawCounts1["Raw counts"]
    BCL --> Demux --> Fastq
    Fastq & Ref --> Mapping --> RawDir --> Convert --> RawCounts1
```

Demultiplexing refers to a two-step process:

(@) The conversion of the binary base call (BCL) files, output by the sequencing machines, into the text-based FASTQ format.
(@) The sorting of reads into different FASTQ files for different libraries pooled together into a single sequencing run.

In order to perform demultiplexing, several tools have been made available in the [demux](../components/workflows/ingestion/demux.qmd) workflow, where the `--demultiplexer` can be used to choose your demultiplexer of choice. Currently, three options have been made available:

* [bcl2fastq(2)](../components/modules/demux/bcl2fastq.qmd): a legacy tool from Illumina that has been replaced by BCL Convert
* [BCL Convert](../components/modules/demux/bcl_convert.qmd): general demultiplexing software by Illumina. 
* Cellranger's [mkfastq](../components/modules/demux/cellranger_mkfastq.qmd): a wrapper around BCL Convert that provides extra convenience features for the processing of 10X single-cell data.

The alignment of reads from the FASTQ files to an appropriate genome reference is called mapping. The result of the mapping process are tables that count the number of times a read has been mapped to a certain feature and metadata information for the cells (observations) and features. There are different format that can be used to store this information together. Since OpenPipeline uses [MuData](./concepts.qmd#sec-common-file-format) as a common file format throughout its pipelines, a conversion to MuData is included in the mapping pipelines.The choice between workflows for mapping is dependant on your single-cell library provider and technology:

* For DB Genomics libraries, the [BD Rhapsody](../components/workflows/ingestion/bd_rhapsody.qmd) pipeline can be used.
* For 10X based libraries, either [cellranger count](../components/workflows/ingestion/cellranger_mapping.qmd) or [cellranger multi](../components/workflows/ingestion/cellranger_multi.qmd) is provided. For more information about the differences between the two and when to use which mapping software, please consult the [10X genomics website](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/multi#when-to-use-multi).

## Creating a transcriptomics reference
Mapping reads from the FASTQ files to features requires a reference that needs to be provided to the mapping component. Depending on the usecase, you might even need to provide references specific for the modalities that you are trying to analyze. For gene expression data, the reference is a reference genome, together with its appropriate gene annotation. A genome reference is often indexed in order to improve the mapping speed. Additionally, some mapping frameworks provided by the single-cell technology providers require extra preprocessing of the reference before they can be used with their worklow. OpenPipelines provides a [make_reference](../components/workflows/ingestion/make_reference.qmd) that allows you to create references in many formats which can be used to map your reads to.


# Processing a single sample {#sec-single-sample}
Some processing can (or must) be performed without concatenating samples together. Even when having the choice, adding a tool to the single-sample processing is preferred because multiple samples can be processed in parallel, improving the processing speed. In general, the processing is modality specific, meaning that a multi-modal sample is first split into its unimodal counterparts. As described in the [multi-sample processing](#sec-multisample-processing), the resulting files are _not_ merged back together after the single-sample processing is done. Instead, the output files for all samples are gathered per modality and concatenated to create a multi-sample unimodal object.


~~~{.d2 layout=elk}
file_input: Input File{
  shape: page
}

split: Split Modalities {
  shape: parallelogram
}


unimodal_gex: "Unimodal Processing\nGene Expression"{
  shape: parallelogram
}

unimodal_prot: "Unimodal Processing\nAntibody Capture"{
  shape: parallelogram
}


multisample: "To Multi-sample Processing"{
  shape: parallelogram
  style.stroke-dash: 5
}


file_input -> split
split -> unimodal_gex
split -> unimodal_prot
split -> multisample

unimodal_gex -> multisample {
  style.stroke-dash: 3
}

unimodal_prot -> multisample {
  style.stroke-dash: 3
}

style: {
  fill: "#FCFCFC"
}

~~~

## Single-sample Gene Expression Processing {#sec-single-sample-gex}
Single-sample gene expression processing involves two steps: removing cells based on count statistics and flagging observations originating from doublets. 

The removal of cells based on basic count statistics is split up into two parts: first, cells are flagged for removal by [filter_with_counts](../components/modules/filter/filter_with_counts.qmd). It flags observations based on several thresholds:

* The number of genes that have a least a single count. Both a maximum and minimum number of genes for a cell to be removed can be specified.
* The percentage of read counts that originated from a mitochodrial genes. Cells can be filtered based on both a maximum or minimum fraction of mitochondrial genes.
* The minimum or maximum total number of counts captured per cell. Cells with 0 total counts are always removed.

Flagging cells for removal involved adding a boolean column to the `.obs` dataframe. After the cells have been flagged for removal, the cells are actually filtered using [do_filter](../components/modules/filter/do_filter.qmd), which reads the values in `.obs` and removed the cells labeled `True`. This applies the general phylosophy of "separation of concerns": one component is responsible for labeling the cells, another for removing them. This keeps the codebase for a single component small and its functionality testable.

The next and final step in the single-sample gene expression processing is doublet detection using [filter_with_scrublet](../components/modules/filter/filter_with_scrublet.qmd). Like `filter_with_counts`, it will not remove cells but add a column to `.obs` (which have the name `filter_with_scrublet` by default). The single-sample GEX workflow will not remove not be removed during the processing (hence no `do_filter`). Howver, you can choose to remove them yourself before doing your analyses by applying a filter with the column in `.obs` yourself. 

~~~{.d2 layout=elk}
direction: right
file_input: "Input File" {
  shape: page
}

count_filtering: "Count Filtering" {
  shape: parallelogram
}

doublet_removal: "Doublet Removal" {
  shape: parallelogram
}

doublet_removal.filter_with_scrublet {
  shape: parallelogram
}

file_output: "Output File" {
  shape: page
}

count_filtering.filter_with_counts {
    shape: parallelogram
}

count_filtering.do_filter {
    shape: parallelogram
}

file_input -> count_filtering
count_filtering -> doublet_removal
doublet_removal -> file_output


style: {
  fill: "#FCFCFC"
}
~~~

## Single-sample Antibody Capture Processing {#sec-single-sample-adt}
The process of filtering antibody capture data is similar to the filtering in [the single-sample gene-expression processing](#sec-single-sample-gex), but without doublet detection. In some particular cases you can use your ADT data to perform doublet detection using for example cell-type maskers. More information can be found in [the single-cell best practices book](https://www.sc-best-practices.org/surface_protein/doublet_detection.html).

~~~{.d2 layout=elk}
direction: right
file_input: "Input File" {
  shape: page
}

count_filtering: "Count Filtering" {
  shape: parallelogram
}

file_output: "Output File" {
  shape: page
}

count_filtering.filter_with_counts {
    shape: parallelogram
}

count_filtering.do_filter {
    shape: parallelogram
}

file_input -> count_filtering
count_filtering -> file_output


style: {
  fill: "#FCFCFC"
}

~~~

# Multisample Processing {#sec-multisample-processing}
After the processing of individual samples has been concluded, samples can be concatenated for further processing. Like with the single-sample processing the multisample processing is not performed on multimodal objects, but each modality separately in order to tailor for the specific modality in question. This means that the result from the singlesample processing is merged together per-modality to create unimodal multisample objects. After processing each modality, all of the modalities can finally be merged and a single object is created that is ready for the integration.

~~~{.d2 layout=elk}
direction: down


input_gex_2: "Processed Sample 2\nGene Expression" {
  shape: page
}
input_adt_2: "Processed Sample 1\nAntibody Capture" {
  shape: page
}

input_gex_1: "Processed Sample 1\nGene Expression" {
  shape: page
}
input_adt_1: "Processed Sample 2\nAntibody Capture" {
  shape: page
}

input_other_1: "Processed Sample 1\nOther Modality" {
  shape: page
}
input_other_2: "Processed Sample 2\nOther Modality" {
  shape: page
}


concat_gex: "Concatenate\nGene Expression" {
  shape: parallelogram
}

concat_adt: "Concatenate\nAntibody Capture" {
  shape: parallelogram
}

concat_other: "Concatenate\nOther Modality" {
  shape: parallelogram
}

multisample_gex: "Multisample Processing\nGene Expression" {
  shape: parallelogram
}

multisample_adt: "Multisample Processing\nAntibody Capture" {
  shape: parallelogram
}

merge {
  shape: parallelogram
}

integration: "To Integration"{
  shape: parallelogram
  style.stroke-dash: 5
}

input_gex_1 -> concat_gex: "unimodal\nsingle-sample"
input_gex_2 -> concat_gex: "unimodal\nsingle-sample"
input_adt_1 -> concat_adt: "unimodal\nsingle-sample"
input_adt_2 -> concat_adt: "unimodal\nsingle-sample" 
input_other_1 -> concat_other: "unimodal\nsingle-sample"
input_other_2 -> concat_other: "unimodal\nsingle-sample"
concat_gex -> multisample_gex: unimodal multi-sample
concat_adt -> multisample_adt: unimodal multi-sample
concat_other -> merge: unimodal multi-sample
multisample_gex -> merge
multisample_adt -> merge
merge -> integration: "multimodal multisample" {
  style.stroke-dash: 3
}

style: {
  fill: "#FCFCFC"
}

~~~


## Multisample Gene Expression Processing
Processing multisample gene expression involved the following steps:

1. [Normalization](../components/modules/transform/normalize_total.qmd): Normalization aims to adjust the raw counts in the dataset for variable sampling effects by scaling the observable variance to a specified range. There are different ways to transform the data, but the normalization method is to make sure each observation (cell) has a total count equal to the median of total counts over all genes for observations (cells) before normalization.
2. [Log transformation](../components/modules/transform/log1p.qmd): Calculates $X = ln(X + 1)$, which converts multiplicative relative changes to additive differences. This allows for interpreting the gene expression in terms of relative, rather than absolute, abundances of genes. 
3. [Highly variable gene detection](../components/modules/filter/filter_with_hvg.qmd): Detects genes that have a large change in expression between samples. By default, OpenPipeline uses the method from Seurat [(Satija et al.)](https://doi.org/10.1038/nbt.3192). As with other "filtering" components, the `filter_with_hvg` component does not remove features, but rather annotates genes of interest by adding a boolean column to `.var`.
4. [QC metric calculations](../components/modules/qc/calculate_qc_metrics.qmd)

~~~{.d2 layout=elk height=1000px}
direction: down

input: "Input" {
  shape: page
}

output: "Output" {
  shape: page
}

normalize: "Normalization" {
  shape: parallelogram
}

log: "Log Transformation" {
  shape: parallelogram
}

filter_with_hvg: "Highly Variable\nGene Detection" {
  shape: parallelogram
}

qc_metrics: "Calculating QC Metrics" {
  shape: parallelogram
}

input -> normalize -> log -> filter_with_hvg -> qc_metrics -> output

style: {
  fill: "#FCFCFC"
}

~~~


## Multisample Antibody Capture Processing
Processing the ADT modality for multiple samples 

~~~{.d2 layout=elk pad=0}
direction: right

input: "Input" {
  shape: page
}

output: "Output" {
  shape: page
}

normalize: "Normalization" {
  shape: parallelogram
}


qc_metrics: "Calculating QC Metrics" {
  shape: parallelogram
}

input -> normalize -> qc_metrics -> output

style: {
  fill: "#FCFCFC"
}
~~~

# Integration {#sec-intergration}

## Dimensionality Reduction {#sec-dimensionality-reduction}
scRNA-seq is a high-throughput sequencing technology that produces datasets with high dimensions in the number of cells and genes. It is true that the data should provide more information, but it also contains more noise and redudant information, making it harder to distill the usefull information. The number of genes and cells can already reduced by gene filtering, but further reduction is a necessity for downstream analysis. Dimensionality reduction projects high-dimensional data into a lower dimensional space (like taking a photo (2D) of some 3D structure). The lower dimensional representation still captures the underlying information of the data, while having fewer dimensions.

Several dimensionality reduction methods have been developed and applied to single-cell data analysis. Two of which are being applied in OpenPipeline:

1. [Principal Component Analysis (PCA)](../components/modules/dimred/pca.qmd): PCA reduces the dimension of a dataset by creating a new set of variables (principal components, PCs) from a linear combination of the original features in such a way that they are as uncorrelated as possible. The PCs can be ranked in the order by which they explain the largest variability in the original dataset. By keeping the top _n_ PCs, the PCs with the lowest variance are discarded to effectively reduce the dimensionality of the data without losing information.
2. [Uniform manifold approximation and projection (UMAP)](../components/modules/dimred/umap.qmd): a non-linear dimensionality technique. It constructs a high dimensional graph representation of the dataset and optimizes the low-dimensional graph representation to be structurally as similar as possible to the original graph. In a review by [Xiang et al., 2021](https://doi.org/10.3389/fgene.2021.646936) it showed the highest stability and separates best the original cell populations.
3. t-SNE is another popular non-linear, graph based dimensionality technique which is very similar to UMAP, but it has not yet been implemented in OpenPipeline.

## Initializing integration {#sec-initializing-integration}
As will be descibed in more details [later on](#sec-integration-methods), many integration methods exist and therefore there is no single integration which is executed by default. However, there are common tasks which are run before integration either because they provide required input for many downstream integration methods or because they popular steps that would otherwise be done manually. These operations _are_ executed by default when using the "full pipeline" as part of the [initialize_integration](../components/workflows/integration/initialize_integration/initialize_integration.qmd) subworkflow.

[PCA](../components/modules/dimred/pca.qmd) is used to reduce the dimensionality of the dataset [as described previously](#sec-dimensionality-reduction). [Find Neighbors](../components/modules/neighbors/find_neighbors.qmd) and [Leiden Clustering](../components/modules/cluster/leiden.qmd) are useful for the identification of cell types or states in the data. Here we apply a popular method to accomplish this is to first calculate a neighborhood graph on a [low dimensinonal representation](#sec-dimensionality-reduction) of the data and then cluster the data based on similarity between data points. Finally, [UMAP](../components/modules/dimred/umap.qmd) allows us to visualise the clusters by reducing the dimensionality of the data while still providing an accurate representation of the underlying cell population structure. 

~~~{.d2 layout=elk pad=0}
direction: right

input: "Input" {
  shape: page
}

output: "Output" {
  shape: page
}

pca: "PCA" {
  shape: parallelogram
}

find_neighbors: "Find\nNeighbors" {
  shape: parallelogram
}

umap: "UMAP" {
  shape: parallelogram
}

input -> pca -> find_neighbors -> umap -> output

style: {
  fill: "#FCFCFC"
}

~~~

## Integration Mehods {#sec-integration-methods}
Integration is the alignment of cell types across samples. There exist three different types of integration methods, based on the degree of integration across modalities:

1. Unimodal integration across batches. For example: [scVI](../components/modules/integrate/scvi.qmd), [scanorama](../components/modules/integrate/scanorama.qmd), [harmony](../components/modules/integrate/harmonypy.qmd)
2. Multimodal integration across batches and modalities. Can be used to integrate joint-profiling data where multiple modalities are measured. For example: [totalVI](../components/modules/integrate/totalvi.qmd)
3. Mosaic integration: data integration across batches and modalities where not all cells are profiled in all modalities and it may be the case that no cells contain profiles in all integrated modalities. Mosaic integration methods have not been made available in OpenPipeline yet. An example of a tool that performs mosaic integration is StabMap.

In either of the three cases, concatenated samples are required, and merged modalities preferred. A plethora of integration methods exist, which in turn interact with other functionality (like clustering and dimensionality reduction methods) to generate a large number of possible usecases which one pipeline cannot cover in an easy manner. Therefore, there is no single integration step that is part of a global pipeline which is executed by default. Instead, a user can choose from the integration workflows provided, and 'stack' integration methods by adding the outputs to different output slots of the MuData object. The following sections will descibe the integration workflows that are available in OpenPipeline.

### Unimodal integration
For unimodal integration, [scVI](../components/modules/integrate/scvi.qmd), [scanorama](../components/modules/integrate/scanorama.qmd) and [harmony](../components/modules/integrate/harmonypy.qmd) have been added to the [scvi_leiden](../components/workflows/integration/scvi_leiden.qmd), [scanorama_leiden](../components/workflows/integration/scanorama_leiden.qmd), and [harmony_leiden](../components/workflows/integration/harmony_leiden.qmd) workflows respectively. After executing the integration methods themselves, [Find Neighbors](../components/modules/neighbors/find_neighbors.qmd) and [Leiden Clustering](../components/modules/cluster/leiden.qmd) are run the results of the integration as wel as [UMAP](../components/modules/dimred/umap.qmd) in order to be able to visualise the results. The functioning of these components has already been described [here](#sec-initializing-integration).

~~~{.d2 layout=elk pad=0}
direction: right

input: "Input" {
  shape: page
}

output: "Output" {
  shape: page
}

integration: "Integration" {
  shape: parallelogram
}

integration.scvi: "scVI" {
  shape: parallelogram
}

integration.scanorama: "Scanorama" {
  shape: parallelogram
}

integration.harmony: "Harmony" {
  shape: parallelogram
}

find_neighbors: "Find\nNeighbors" {
  shape: parallelogram
}

leiden_clustering: "Leiden\nClustering" {
  shape: parallelogram
}

umap: "UMAP" {
  shape: parallelogram
}

input -> integration -> find_neighbors -> leiden_clustering -> umap -> output

style: {
  fill: "#FCFCFC"
}

~~~

### Multimodal Integration
A single multimodal integration method is currently avaiable in OpenPipeline: [totalVI](../components/modules/integrate/totalvi.qmd). It allows using information from both the gene-expression data and the antibody-capture data together to integrate the cell types. As with the other integration workflows, after running totalVI, [Find Neighbors](../components/modules/neighbors/find_neighbors.qmd), [Leiden Clustering](../components/modules/cluster/leiden.qmd) and [UMAP](../components/modules/dimred/umap.qmd) are run on the result. However in this case the three components are executed on both of the integrated modalities.

~~~{.d2 layout=elk pad=0}
direction: right

input: "Input" {
  shape: page
}

output: "Output" {
  shape: page
}

integration: "Integration" {
  shape: parallelogram
}

integration.totalvi: "TotalVI" {
  shape: parallelogram
}

find_neighbors: "Find\nNeighbors" {
  shape: parallelogram
}

leiden_clustering: "Leiden\nClustering" {
  shape: parallelogram
}

umap: "UMAP" {
  shape: parallelogram
}

input -> integration -> find_neighbors -> leiden_clustering -> umap -> output

style: {
  fill: "#FCFCFC"
}

~~~


# Putting it all together: the "Full Pipeline" {#sec-full-pipeline}

:::{.column-screen-inset-shaded}

```{mermaid}
%%| label: fig-architecture
%%| fig-cap: Overview single cell processing steps in OpenPipeline. Rectangles are data objects, parallelograms are Viash modules or subworkflows.


flowchart TB
  Raw1[/Sample 1/]:::file --> Split
  Raw2[/Sample 2/]:::file --> Split2
  subgraph FullPipeline [Full Pipeline]
    NoIntegration -.-> MultimodalFile[/Multisample\nMultimodal File/]:::file -.-> MultiSample
    Split([Split\nmodalities]):::component --gex modality--> ProcGEX1
    Split([Split\nmodalities]):::component --prot modality--> ProcADT1
    Split([Split\nmodalities]):::component -- other--> ConcatVDJ

    Split2([Split\nmodalities]):::component --gex modality--> ProcGEX1 
    Split2([Split\nmodalities]):::component --prot modality--> ProcADT1
    Split2([Split\nmodalities]):::component -- other--> ConcatVDJ
    

    subgraph MultiSample [Multisample]
      subgraph MultisampleRNA [Multisample RNA]
      end
      MultisampleRNA:::workflow
      subgraph MultisampleADT [Multisample ADT]
      end
      MultisampleADT:::workflow
      subgraph Unknown["Untreated modality (e.g. VDJ)"]
      end
      Unknown:::logicalgrouping
    end
    ConcatVDJ([Concatenate]):::component
    NoIntegration[Multisample Multimodality]

    ConcatVDJ([Concatenate]):::component --> Unknown
    ConcatGEX([Concatenate]):::component --> MultisampleRNA
    ConcatADT([Concatenate]):::component --> MultisampleADT

    MultisampleRNA & MultisampleADT & Unknown--> Merge([Merge\nmodalities]):::component --> NoIntegration:::workflow
    
  end

  subgraph Wrapper
    subgraph Integration[Integration]
      subgraph IntegrationRNA[Integration RNA]
          direction LR
          hamony_leiden_rna[Harmony + Leiden]:::workflow
          scvi_rna[scVI + Leiden]:::workflow
          scanorama[Scanorama + Leiden]:::workflow
          other[...]:::workflow
      end
      subgraph IntegrationProt[Integration ADT]
          direction LR
          hamony_leiden_prot[Harmony + Leiden]:::workflow
          otherprot[...]:::workflow
      end
      subgraph multimodal_integration[Multimodal integration]
          totalVI([totalVI]):::component
      end
      multimodal_integration:::logicalgrouping
      IntegrationRNA:::logicalgrouping --choose from--> IntegrationProt:::logicalgrouping
      NoIntegration ---> totalVI
    end
    Integration:::logicalgrouping
    subgraph LegendBox[Legend]
      direction LR
      component([component]):::component
      multiple_component((Multiple Components)):::component
      workflow["(Sub)workflow"]:::workflow
      file[/file/]:::file
      Logicalgrouping[Logical grouping]:::logicalgrouping
    end
  LegendBox:::legend
  end
  Wrapper:::hide


  NoIntegration --choose from--> IntegrationRNA
  %% NoIntegration ~~~ LegendBox

  ProcGEX1[Process GEX\nSingle Sample]:::workflow --> ConcatGEX
  ProcADT1[Process ADT\nSingle Sample]:::workflow --> ConcatADT



  style FullPipeline fill: #5cc,font-size:1.4em,color:#000;
  classDef hide fill:transparent,color:transparent,stroke:transparent;
  classDef legend fill:transparent;
  classDef file fill: #5c5c5c,color:#fff,stroke-dasharray: 5 5;
  classDef logicalgrouping fill:transparent,stroke-dasharray: 5 5;
  classDef workflow fill:#ffffde,color:#000;
  classDef component fill:#ececff,color:#000;

```

:::
